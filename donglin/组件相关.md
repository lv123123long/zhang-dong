# flink
* 提交flink作业简单测试
/opt/xinghai/flink/software/flinkonyarn/bin/flink run -m yarn-cluster /opt/xinghai/flink/software/flinkonyarn/examples/batch/WordCount.jar

* 组件测试需要放在安装该组件下面执行
![[lx_clip1630028894185.png]]
* ./flink run ../examples/batch/WordCount.jar --input hdfs://10.48.92.211:9000/LICENSE --output hdfs://10.48.92.211:9000/wordcount-result.txt
* 这个是自己写的input 和output命令，，LICENSE 和wordcount-result.txt都是自己随便创的，然后用./hadoop fs -put  上传到hdfs的/目录的
```
./flink run -m yarn-cluster ../examples/batch/WordCount.jar --input hdfs://10.48.92.211:9000/LICENSE --output hdfs://10.48.92.211:9000/wordcount-result.txt

./flink run -m yarn-cluster ../examples/batch/WordCount.jar --input hdfs:///LICENSE --output hdfs:///user/res02
```

## ES
```
查看某个索引中的所有数据
curl -u es_admin:%36.Hadoop* -X GET "localhost:9200/_sql" -H 'Content-Type: application/json' -d 'select * from index_10'| python -m json.tool

curl -u es_admin:%36.Hadoop* -XGET localhost:9200/_cat/indices

```