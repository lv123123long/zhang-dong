* 网络探测器、Crawler爬行器、Spider蜘蛛、Robot机器人等，其中网络爬虫或网络蜘蛛的叫法会更形象更生动一些，取意为网页爬取程序像虫子和蜘蛛一样在网络间爬来爬去，从一个网页链接爬到另一个网页链接。世界上第一个网络爬虫是MIT Matthew Gray的World Wide Web Wanderer，Wanderer主要用于追踪互联网发展规模。
* 网络蜘蛛在工作时，通过种子爬取网页的链接地址来寻找目标网页。随后从网站的1个页面，如首页，开始读取网页的内容和网页中其他网页的链接地址，然后通过这些链接地址继续寻找下一个网页。如此循环，直到所有内容都被抓取完成。
* 网络爬虫爬取过程中，为了提高爬取效率，一般采用并行爬取的方式。多个网络爬虫在并行爬取过程中，不重复爬取同一个网页尤为重要，这将极大地提高爬取效率。
* 搜索引擎会建立两张不同的表，一张表记录已经访问过的网址，另一张表记录没有访问过的网址
* 网络爬虫在按照链接爬取网页的过程中，网页之间的关系有点类似于有向图。在有向图的节点遍历过程中，我们可以按照“先深度后广度”的方式遍历，也可以按照“先广度后深度”的方式遍历。同样，在网络爬虫爬取网页的过程中，网络爬虫需要根据一定的策略来爬取网页，一般采用“先深度后广度”的方式。
## 网页的收录模式
### 增量收集
* 增量收集可以避免全量收集模式的弊端，这种模式主要用于搜集新网页、搜集更新的网页，删除不存在的页面。当然，相较于全量收集，网络爬虫的系统设计也会复杂一些，但时效性好。
### 全量搜集
* 全量搜集，顾名思义，每次爬取网页都更新全部数据内容。这种模式一般定期展开，因为全量搜集模式的资源开销大、付出成本高、内容更新的时效性不高、网络宽带消耗高，而且更新全量数据所需时间也比较长。

## 结论
网络爬虫的工作核心就在于在网页搜集效率、质量和对目标网站的友好程度上。网络爬虫要用最少的资源、最少的时间，搜集尽可能多的高质量网页；同时对目标网站的内容抓取不影响网站的正常运转和使用。
Java语言栈的读者可以使用WebMagic、Gecco；Python语言栈的读者可以使用Scrapy；Go语言栈的读者可以使用YiSpider。

## 网页分析

* 网页分析程序将自动对网页进行分析。主要的分析动作有网页内容摘要、链接分析、网页重要程度计算、关键词提取/分词、去除噪声等。经过网页分析后，网页数据将变成网页中关键词组、链接与关键词的相关度、网页重要程度等信息。
* 网页内容中的去除噪声主要是去除如广告、无关的导航条、版权信息、调查问卷等和文章主体内容无关的内容。
* 网页重要程度计算用于衡量网站的权威性。一般越权威的网站，越容易被其他网站主动链接；换言之，网站被引用的次数越多，说明该网站越重要。对搜索引擎而言，在返回相关性强的内容时，应该尽量先返回权威网站的内容；对搜索引擎的用户而言，这样往往更能匹配他们的需要。因此这也是评价搜索引擎体验好坏的核心指标之一。
*
### 关键字提取环节
* 在关键词提取/分词环节，基础技术是分词，**在创建索引之前都需要对内容进行分词。分词不仅是关键词提取的前提，也是后续文本挖掘的基础。**
* 在中文分词时，常用的算法可以分为两大类，一类是基于字典的机械式分词，另一类是基于统计的分词。
### 基于字典的分词方法，
一般会按照一定的策略将待分析的汉字串与一个充分大的词典的词条进行匹配，若在词典中找到某个字符串词条，则匹配成功。因此基于字典的分词方法的核心字符串的匹配。
* 在匹配字符串时分为正向匹配和逆向匹配两种。
* 正向匹配指的是在匹配字符串时从左向右匹配。如“中国”“中国人”两个词条字符串在匹配时，从左向右可以依次匹配成功“中”和“国”。而逆向匹配与之相反，一般是从右向左匹配。
* 中文的特性，多个词条往往有相关的前缀或后缀，如“中国”“中国人”“中国话”三个词条都有“中国”这个公共前缀。因此在某方向，如正向或逆向匹配过程中，按匹配长度的不同，还可以细分为最大/最长匹配和最小/最短匹配。我们仍然以“中国”“中国人”“中国话”三个词条举例，正向最短匹配的结果是“中”，而正向最长匹配的结果是“中国”。
### 基于统计的分词算法。
* 根据汉字与汉字相邻出现的概率来进行分词。因此基于统计的分词算法往往需要构建一个语料库，并不断更新。在分词前，算法需要进行预处理，即对语料库中相邻出现的各个字的组合进行统计，计算两个汉字间的组合概率。

不论是哪类分词算法，都会对新词不敏感，都会分出一些经常出现但并非有效词组的内容，如“有点”“有的”“好像”“非常”等。因此有必要对这类词语进行过滤，毕竟分词算法最主要的指标就是分词的准确率。

**Python语言中的中文分词组件有jieba中文分词，Java语言中常用Jcseg、Ansj和庖丁分词，Go语言中常用sego。**

